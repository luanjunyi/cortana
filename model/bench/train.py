# This is a most naive algorithm. For each sentence it assumes each word is generated by the
# domain it belongs to. It simply predict the target domain d as the one that maximize the value of:
# P(w1|d) * P(w2|d) * ... * P(wn|d)
# Use backoff discounting for words never occured in domain d. Ignore words never seen in the 
# training data.

import sys, os, math
import argparse
import cPickle as pickle
from collections import defaultdict

UTIL = os.path.abspath(os.path.dirname(os.path.abspath(__file__)) + "/../../util")
sys.path.append(UTIL)

from log import _logger

class NaiveBayes(object):
    def __init__(self, train_path, term_path, alpha = 0.5):
        self.train_path = train_path
        self.term_path = term_path
        self.term_count = defaultdict(int)
        self.count = defaultdict(int)
        self.domain_has = defaultdict(set)
        self.domain_backoff = dict()
        self.load_terms()
        self.alpha = alpha

    def load_terms(self):
        _logger.info("Loading terms from %s" % self.term_path)
        with open(self.term_path) as term_file:
            for line in term_file:
                line = line.strip()
                if line:
                    term, count, _ = line.split(' ')
                    count = int(count)
                    self.term_count[term] = count
        _logger.info("%d terms read from file" % len(self.term_count))

    def train(self):
        _logger.info("Training data from %s" % self.train_path)
        with open(self.train_path) as train_file:
            for line in train_file:
                line = line.strip()
                if not line:
                    continue
                terms, domain = line.split('\t')
                terms = terms.split(' ')
                for term in terms:
                    self.count[term, domain] += 1
                    self.count[domain] += 1
                    self.domain_has[domain].add(term)

        for domain in self.domain_has:
            backoff = len(self.domain_has[domain]) * 0.5 / self.count[domain]
            backoff /= len(self.term_count) - len(self.domain_has[domain])
            self.domain_backoff[domain] = backoff

if __name__ == "__main__":
    cmd = argparse.ArgumentParser()
    cmd.add_argument("--input", help="path of the training data")
    cmd.add_argument("--terms", help="path of the terms file")
    cmd.add_argument("--alpha", help="alpha of discounting")

    args = cmd.parse_args()
    naive = NaiveBayes(args.input, args.terms, args.alpha)
    naive.train()
    with open("naive.model", "w") as outfile:
        pickle.dump(naive, outfile)
    _logger.info("Model dumped to naive.model")
